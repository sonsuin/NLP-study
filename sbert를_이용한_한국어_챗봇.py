# -*- coding: utf-8 -*-
"""SBERT를 이용한 한국어 챗봇

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BYgdUCtbd4WGvuYinRyGGAtW7oV7aEuE
"""

# SBERT를 이용하여 문장 임베딩을 얻을 수 있는 패키지 sentence_transformers 설치
pip install sentence_transformers

import numpy as np
import pandas as pd
from numpy import dot
from numpy.linalg import norm
import urllib.request
from sentence_transformers import SentenceTransformer

urllib.request.urlretrieve("https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv", filename="ChatBotData.csv")
train_data = pd.read_csv('ChatBotData.csv')
train_data.head()

# 문장 임베딩을 얻기 위해 사전 훈련된 BERT 로드
# 여기선 한국어 포함되어 학습된 다국어 모델 로드

# 모델 이름 : 'xlm-r-100langs-bert-base-nli-stsb-mean-tokens'
# 이름이 의미하는 바는 100가지 언어(한국어포함)지원하는 다국어 BERT BASE 모델
# SNLI데이터 학습 후 STS-B 데이터로 학습
# 문장 표현을 얻기 위해 평균풀링(mean-tokens) 사용
# -> NLI 데이터를 학습 후 STS데이터로 추가 파인튜닝한 모델

model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')

# 데이터의 모든 질문열 즉 train_data['Q']에 대해 문장 임베딩 값 구한 후 embedding이라는 새로운 열에 저장

train_data['embedding'] = train_data.apply(lambda row: model.encode(row.Q), axis = 1)

# 두 개의 벡터로부터 코사인 유사도 구하는 함수 정의

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

# 임의의 질문이 들어오면 
# 해당 질문의 문장 임베딩 값과 
# 챗봇 데이터의 임베딩 열, 즉 train_data['embedding']에 저장해둔 모든 질문 샘플들의 문장 임베딩 값들을
# 전부 비교 후 코사인 유사도 값이 가장 높은 질문 샘플 추출

# 해당 질문 샘플과 짝이 되는 답변 샘플 리털

def return_answer(question):
    embedding = model.encode(question)
    train_data['score'] = train_data.apply(lambda x: cos_sim(x['embedding'], embedding), axis=1)
    return train_data.loc[train_data['score'].idxmax()]['A']

# 챗봇 테스트

return_answer('나랑 밥먹자')

return_answer('나 오늘 심심해')

return_answer('나랑 같이 공부하자')